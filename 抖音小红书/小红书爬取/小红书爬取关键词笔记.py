#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦Seleniumé›†æˆæ–¹æ¡ˆ
ç»“åˆSeleniumç™»å½•çŠ¶æ€ä¿æŒ + APIè¯·æ±‚å¤´è·å– + ç°æœ‰çˆ¬å–é€»è¾‘
æ ¸å¿ƒç›®æ ‡ï¼šåªéœ€ç™»å½•ä¸€æ¬¡ï¼Œåç»­æ‰€æœ‰æ“ä½œéƒ½ä¿æŒç™»å½•çŠ¶æ€
"""

import os
import time
import json
import requests
from urllib.parse import urlencode
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
from typing import Dict, List, Any, Tuple
import urllib.parse
import sys
import warnings
import re
import threading
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed

# ç¦ç”¨æ‰€æœ‰è­¦å‘Šä¿¡æ¯
warnings.filterwarnings("ignore")

# ç¦ç”¨urllib3çš„SSLè­¦å‘Š
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç§»é™¤é’ˆå¯¹ absl/voice çš„ç‰¹æ®ŠæŠ‘åˆ¶é€»è¾‘ï¼Œä¿ç•™é»˜è®¤è¾“å‡º

class XHSIntegratedCrawler:
    """å°çº¢ä¹¦é›†æˆçˆ¬è™«ï¼šSeleniumç™»å½•çŠ¶æ€ä¿æŒ + APIè¯·æ±‚"""
    
    def __init__(self):
        self.driver = None
        self.headers: Dict[str, str] = {}
        self.cookies: Dict[str, str] = {}
        self.last_signed_headers: Dict[str, str] = {}
        self.filter_signed_headers: Dict[str, str] = {}
        # æ–¹æ¡ˆBï¼šä»…å½“å‘½ä¸­ /search/filter æ—¶ï¼Œè®°å½•åŒæº search_id ä¸ keyword
        self.filter_search_id: str = ''
        self.filter_keyword: str = ''
        self.filter_captured_at: float = 0.0
        self.user_data_dir = os.path.join(os.getcwd(), "chrome_user_data")
        self.official_search_id = None
        self.cached_filter_groups: List[Dict[str, Any]] = []
        self._printed_search_id: bool = False
        
        # APIç›¸å…³é…ç½®
        self.search_url = "https://edith.xiaohongshu.com/api/sns/web/v1/search/notes"
        
        
    def _setup_browser(self):
        """é…ç½®å¹¶å¯åŠ¨Chromeæµè§ˆå™¨ï¼Œä¿æŒç™»å½•çŠ¶æ€"""
        chrome_options = Options()
        
        # æ ¸å¿ƒï¼šè®¾ç½®ç”¨æˆ·æ•°æ®ç›®å½•ï¼Œä¿å­˜ç™»å½•çŠ¶æ€
        chrome_options.add_argument(f'--user-data-dir={self.user_data_dir}')
        chrome_options.add_argument('--profile-directory=Default')
        
        # åŸºæœ¬è®¾ç½®
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # ç¦ç”¨å„ç§æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯
        chrome_options.add_argument('--disable-logging')
        chrome_options.add_argument('--disable-log-file')
        chrome_options.add_argument('--log-level=3')  # åªæ˜¾ç¤ºè‡´å‘½é”™è¯¯
        chrome_options.add_argument('--silent')
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-features=TranslateUI')
        chrome_options.add_argument('--disable-ipc-flooding-protection')
        chrome_options.add_argument('--disable-component-update')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-domain-reliability')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor')
        
        # ç¦ç”¨DevToolså’Œæ›´å¤šæ—¥å¿—
        # ä¿æŒDevToolså¯ç”¨ä»¥ç¨³å®šäº§ç”Ÿ *ExtraInfo äº‹ä»¶
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-infobars')
        # ä¸å†ç¦ç”¨web securityï¼Œé¿å…å½±å“å‡­è¯/ç­–ç•¥
        chrome_options.add_argument('--disable-features=VizDisplayCompositor,AudioServiceOutOfProcess,VoiceTranscription')
        chrome_options.add_argument('--disable-speech-api')
        chrome_options.add_argument('--disable-background-mode')
        chrome_options.add_argument('--disable-background-downloads')
        chrome_options.add_argument('--disable-client-side-phishing-detection')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-voice-input')
        chrome_options.add_argument('--disable-speech-synthesis-api')
        chrome_options.add_argument('--disable-media-session-api')
        
        # å¯ç”¨æ€§èƒ½æ—¥å¿—ä»¥æ•è·ç½‘ç»œè¯·æ±‚
        chrome_options.add_experimental_option('perfLoggingPrefs', {
            'enableNetwork': True,
            'enablePage': False,
        })
        chrome_options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
        
        # è®¾ç½®ç”¨æˆ·ä»£ç†
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        try:
            # æŒ‡å®šchromedriverè·¯å¾„
            chromedriver_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "chromedriver.exe")
            if not os.path.exists(chromedriver_path):
                chromedriver_path = os.path.join(os.getcwd(), "chromedriver.exe")
            
            if os.path.exists(chromedriver_path):
                from selenium.webdriver.chrome.service import Service
                service = Service(chromedriver_path, log_path=os.devnull)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            else:
                self.driver = webdriver.Chrome(options=chrome_options)
            
            # å¯ç”¨CDPç½‘ç»œ/é¡µé¢äº‹ä»¶ï¼Œç¡®ä¿å¯ä»¥æ•è·å®Œæ•´è¯·æ±‚å¤´ï¼ˆåŒ…å« ExtraInfo äº‹ä»¶ï¼‰
            try:
                self.driver.execute_cdp_cmd('Network.enable', {
                    'maxTotalBufferSize': 10000000,
                    'maxResourceBufferSize': 10000000,
                    'includeExtraInfo': True  # å…³é”®ï¼šç¡®ä¿èƒ½æ”¶åˆ° *ExtraInfo äº‹ä»¶ä»¥è·å–å®Œæ•´è¯·æ±‚å¤´
                })
                self.driver.execute_cdp_cmd('Page.enable', {})
            except Exception:
                pass

            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            print("âœ… Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            print(f"ğŸ“ ç”¨æˆ·æ•°æ®ç›®å½•: {self.user_data_dir}")
            return True
        
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥: {e}")
            return False
    
    def _ensure_login(self):
        """æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œå¦‚æœæœªç™»å½•åˆ™å¼•å¯¼ç™»å½•"""
        try:
            # å¯¼èˆªåˆ°å°çº¢ä¹¦é¦–é¡µ
            print("ğŸŒ æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            self.driver.get("https://www.xiaohongshu.com")
            time.sleep(3)
            
            # ä»…ä½¿ç”¨ UI å…ƒç´ å­˜åœ¨æ€§ä½œä¸ºç™»å½•åˆ¤å®šï¼š//*[@id="global"]/div[2]/div[1]/ul/div[1]/div[1]
            if self._is_logged_in_ui(timeout=3.0):
                print("âœ… æ£€æµ‹åˆ°å·²æœ‰ç™»å½•çŠ¶æ€ï¼Œæ— éœ€é‡æ–°ç™»å½•")
                return True

            # æœªæ£€æµ‹åˆ°ç™»å½•å…ƒç´ ï¼šæç¤ºç™»å½•å¹¶ç­‰å¾…è‡³æ£€æµ‹é€šè¿‡
            print("â„¹ï¸ éœ€è¦ç™»å½•ï¼šè¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œå®Œæˆåå›åˆ°æ§åˆ¶å°ç»§ç»­")
            input("\næŒ‰å›è½¦é”®ç¡®è®¤å·²å®Œæˆç™»å½•...")

            # è½®è¯¢ç­‰å¾…è¯¥å…ƒç´ å‡ºç°
            for _ in range(60):  # æœ€å¤šçº¦30ç§’
                if self._is_logged_in_ui(timeout=0.5):
                    print("âœ… ç™»å½•æˆåŠŸï¼Œä¼šè¯å·²æŒä¹…åŒ–è‡³ç”¨æˆ·æ•°æ®ç›®å½• chrome_user_data/")
                    return True
                time.sleep(0.5)

            # å…œåº•ï¼šåˆ·æ–°ä¸€æ¬¡å†çŸ­è½®è¯¢
            try:
                self.driver.refresh()
            except Exception:
                pass
            time.sleep(1.0)
            for _ in range(20):
                if self._is_logged_in_ui(timeout=0.5):
                    print("âœ… ç™»å½•æˆåŠŸï¼Œä¼šè¯å·²æŒä¹…åŒ–è‡³ç”¨æˆ·æ•°æ®ç›®å½• chrome_user_data/")
                    return True
                time.sleep(0.5)

            # æ”¾è¡Œä½†æç¤º
            print("âš ï¸ ä»æœªæ£€æµ‹åˆ°ç™»å½•å…ƒç´ ï¼Œç»§ç»­æ‰§è¡Œï¼ˆè‹¥åç»­å¤±è´¥è¯·å®Œæˆç™»å½•åé‡è¯•ï¼‰")
            return True
        
        except Exception as e:
            print(f"âŒ ç™»å½•æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _is_logged_in_ui(self, timeout: float = 2.0) -> bool:
        """æ ¹æ®æ–°è§„åˆ™åˆ¤æ–­æ˜¯å¦å·²ç™»å½•ï¼š
        - å­˜åœ¨ XPath //*[@id='app']/div[1]/div/i => æœªç™»å½•
        - ä¸å­˜åœ¨è¯¥å…ƒç´  => å·²ç™»å½•
        è¿”å› True è¡¨ç¤ºâ€œå·²ç™»å½•â€ã€‚
        """
        xpath_marker = '//*[@id="app"]/div[1]/div/i'
        try:
            # ä½¿ç”¨çŸ­ç­‰å¾…æŸ¥æ‰¾å…ƒç´ æ˜¯å¦å‡ºç°ï¼›å‡ºç°åˆ™æœªç™»å½•
            elems = self.driver.find_elements(By.XPATH, xpath_marker)
            if elems and len(elems) > 0:
                return False  # æœªç™»å½•
            # è‹¥æœªç«‹å³æ‰¾åˆ°ï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´å†æ¬¡ç¡®è®¤
            end = time.time() + timeout
            while time.time() < end:
                elems = self.driver.find_elements(By.XPATH, xpath_marker)
                if elems and len(elems) > 0:
                    return False
                time.sleep(0.1)
            return True  # è¶…æ—¶ä»æœªå‘ç°è¯¥å…ƒç´ ï¼Œè§†ä¸ºå·²ç™»å½•
        except Exception:
            # å¼‚å¸¸æ—¶ä¿å®ˆè¿”å› Falseï¼ˆæœªç™»å½•ï¼‰ï¼Œé¿å…è¯¯æ”¾è¡Œ
            return False
    
    def _perform_search_via_ui(self, keyword: str) -> bool:
        """åœ¨é¦–é¡µé€šè¿‡UIæ‰§è¡Œæœç´¢ï¼Œç¡®ä¿ä¸ç«™å†…æµç¨‹ä¸€è‡´ï¼ˆç”Ÿæˆæ­£ç¡®çš„search_idä¸ç­¾åç»‘å®šï¼‰ã€‚"""
        try:
            self.driver.get("https://www.xiaohongshu.com")
            time.sleep(1.2)

            # å¸¸è§è¾“å…¥æ¡†é€‰æ‹©å™¨é›†åˆï¼Œé€ä¸€å°è¯•
            selectors = [
                "input[placeholder*='æœç´¢']",
                "input[type='search']",
                "input[role='searchbox']",
                "input[name='keyword']",
            ]
            elem = None
            for css in selectors:
                try:
                    elem = WebDriverWait(self.driver, 3).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, css))
                    )
                    if elem:
                        break
                except Exception:
                    continue
            if not elem:
                # é€€è€Œæ±‚å…¶æ¬¡ï¼šç”¨JSåœ¨é¡µé¢å†…æŸ¥æ‰¾ç¬¬ä¸€ä¸ªinputå†å°è¯•
                try:
                    elem = self.driver.execute_script("return document.querySelector('input,textarea')")
                except Exception:
                    elem = None
            if not elem:
                print("âš ï¸ æœªæ‰¾åˆ°æœç´¢æ¡†ï¼Œæ”¹ä¸ºç›´æ¥å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µ")
                self.driver.get(f"https://www.xiaohongshu.com/search_result?keyword={urllib.parse.quote(keyword)}")
                time.sleep(1.2)
                return True

            try:
                elem.clear()
            except Exception:
                pass
            elem.send_keys(keyword)
            try:
                from selenium.webdriver.common.keys import Keys
                elem.send_keys(Keys.ENTER)
            except Exception:
                # å›é€€ï¼šç‚¹å‡»æœç´¢æŒ‰é’®
                try:
                    btn = self.driver.find_element(By.CSS_SELECTOR, "button, [role='button']")
                    btn.click()
                except Exception:
                    pass

            time.sleep(1.5)
            return True
        except Exception as e:
            print(f"âš ï¸ UIæœç´¢è§¦å‘å¤±è´¥: {e}")
            return False

    def _extract_headers_from_browser(self):
        """ä»æµè§ˆå™¨ä¸­æå–è¯·æ±‚å¤´å’ŒCookie"""
        try:
            # è·å–æ‰€æœ‰Cookie
            browser_cookies = self.driver.get_cookies()
            self.cookies = {cookie['name']: cookie['value'] for cookie in browser_cookies}
            
            # è·å–çœŸå®UA
            try:
                real_ua = self.driver.execute_script("return navigator.userAgent") or ""
            except Exception:
                real_ua = ""
            if not real_ua:
                real_ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"

            # æ„å»ºåŸºç¡€è¯·æ±‚å¤´ï¼ˆå‰”é™¤åŠ¨æ€ç­¾åå¤´ä¸ä¼ªé¦–éƒ¨ï¼‰
            self.headers = {
                "accept": "application/json, text/plain, */*",
                "accept-encoding": "gzip, deflate, br, zstd",
                "accept-language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
                "cache-control": "no-cache",
                "content-type": "application/json;charset=UTF-8",
                "origin": "https://www.xiaohongshu.com",
                "pragma": "no-cache",
                "priority": "u=1, i",
                "referer": "https://www.xiaohongshu.com/",
                # ä¿ç•™sec-*ç³»åˆ—ï¼›å…·ä½“å€¼ä»¥æµè§ˆå™¨ä¸ºå‡†ï¼Œè‹¥éœ€æ›´ç²¾å‡†å¯åç»­ä»CDPå–
                "sec-ch-ua": '"Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"',
                "sec-ch-ua-mobile": "?0",
                "sec-ch-ua-platform": '"Windows"',
                "sec-fetch-dest": "empty",
                "sec-fetch-mode": "cors",
                "sec-fetch-site": "same-site",
                "user-agent": real_ua,
            }
            
            # æ·»åŠ Cookieåˆ°è¯·æ±‚å¤´
            cookie_string = '; '.join([f"{name}={value}" for name, value in self.cookies.items()])
            self.headers['cookie'] = cookie_string
            
            print(f"âœ… æˆåŠŸæå– {len(self.cookies)} ä¸ªCookie")
            return True
            
        except Exception as e:
            print(f"âŒ æå–è¯·æ±‚å¤´å¤±è´¥: {e}")
            return False
    
    def _make_api_request(self, url, method='GET', data=None, params=None, override_headers: Dict[str, str] = None):
        """ä½¿ç”¨æå–çš„è¯·æ±‚å¤´è¿›è¡ŒAPIè¯·æ±‚"""
        try:
            # è®©requestsè‡ªåŠ¨å¤„ç†URLç¼–ç ï¼Œä¸æ‰‹åŠ¨ç¼–ç 
            # å‘é€å‰å®‰å…¨è¿‡æ»¤ï¼šå‰”é™¤éæ³•å¤´
            safe_headers = {k: v for k, v in self.headers.items() if isinstance(k, str) and not k.strip().startswith(":")}
            
            # åˆå¹¶æœ€è¿‘æ•è·çš„ç­¾åå¤´ï¼ˆç™½åå•è¦†ç›–ï¼‰
            try:
                if isinstance(self.last_signed_headers, dict) and self.last_signed_headers:
                    # ç­¾åå¤´ç™½åå•
                    signature_keys = ['x-s', 'x-t', 'x-s-common', 'x-b3-traceid', 'x-xray-traceid']
                    for k in signature_keys:
                        v = self.last_signed_headers.get(k)
                        if isinstance(v, str) and v:
                            if not k.strip().startswith(':'):
                                safe_headers[k] = v
            except Exception as e:
                pass

            # è¦†ç›–å¤´ï¼ˆç”¨äºä¸¥æ ¼æŒ‡å®šæ¥æºäº /search/filter çš„å¤´ï¼‰
            if isinstance(override_headers, dict) and override_headers:
                for k, v in override_headers.items():
                    if isinstance(k, str) and isinstance(v, str) and k and not k.strip().startswith(':'):
                        safe_headers[k] = v
            
            if method.upper() == 'POST':
                response = requests.post(url, headers=safe_headers, json=data, params=params, timeout=20)
            else:
                response = requests.get(url, headers=safe_headers, params=params, timeout=20)
            return response.json()
            
        except Exception as e:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    # ä»¥ä¸‹æ˜¯åŸæœ‰çš„çˆ¬å–é€»è¾‘ï¼Œé€‚é…æ–°çš„è¯·æ±‚å¤´ç³»ç»Ÿ
    def _extract_token_id_pairs(self, node: Any) -> List[Tuple[str, str]]:
        """é€’å½’éå† JSONï¼Œæå–åŒ…å« xsec_token ä¸ id çš„å¯¹è±¡å¯¹"""
        results: List[Tuple[str, str]] = []

        def walk(n: Any):
            if isinstance(n, dict):
                has_token = "xsec_token" in n and isinstance(n["xsec_token"], str)
                has_id = "id" in n and isinstance(n["id"], str)
                if has_token and has_id:
                    results.append((n["xsec_token"], n["id"]))
                for v in n.values():
                    walk(v)
            elif isinstance(n, list):
                for it in n:
                    walk(it)

        walk(node)
        return results

    def _sanitize_note_id(self, nid: Any) -> str:
        """æ¸…æ´— IDï¼šè½¬å­—ç¬¦ä¸²ï¼Œå»é™¤ # æˆ– ? ä¹‹åçš„ç‰‡æ®µï¼Œè½¬å°å†™å¹¶å»é™¤é¦–å°¾ç©ºç™½"""
        s = str(nid)
        if '#' in s:
            s = s.split('#', 1)[0]
        if '?' in s:
            s = s.split('?', 1)[0]
        return s.strip().lower()

    def _is_valid_note_id(self, nid: str) -> bool:
        """æ ¡éªŒæ˜¯å¦ä¸º 24 ä½åå…­è¿›åˆ¶ note_idï¼ˆå°çº¢ä¹¦å¸¸è§ï¼‰"""
        note_id_re = re.compile(r"^[0-9a-f]{24}$")
        return bool(note_id_re.fullmatch(nid))

    
    def _extract_master_streams_from_text(self, text: str) -> List[Dict[str, Any]]:
        """ä»é¡µé¢æºç /JSONæ–‡æœ¬ä¸­æå–è§†é¢‘æµä¿¡æ¯ï¼ˆmasterUrlã€backupUrlsã€streamTypeï¼‰ã€‚
        é€‚é…å½¢å¦‚ 1.txt ä¸­çš„ç‰‡æ®µï¼šåŒ…å« "masterUrl": "http:\\u002F...mp4"ï¼Œå¹¶åœ¨é‚»è¿‘åŒºåŸŸæœ‰ "streamType": 114/115/259ã€‚
        è¿”å›åˆ—è¡¨å…ƒç´ ç¤ºä¾‹ï¼š{"streamType": 114, "masterUrl": "http://..._114.mp4", "backupUrls": ["http://...", ...]}
        """
        streams: List[Dict[str, Any]] = []
        if not isinstance(text, str) or not text:
            return streams

        # è§£ç å¸¸è§çš„ \u002F -> /
        safe = text.replace("\\u002F", "/")

        # åŸºäºå¯¹è±¡ç‰‡æ®µçš„ç²—æå–ï¼šä»¥ masterUrl ä¸ºé”šç‚¹ï¼Œå‘å‰åçª—å£æŸ¥æ‰¾ streamType å’Œ backupUrls
        pattern_master = re.compile(r'"masterUrl"\s*:\s*"([^"]+?\.mp4)"', re.IGNORECASE)
        for m in pattern_master.finditer(safe):
            url = m.group(1)
            start, end = m.start(), m.end()
            # åœ¨é™„è¿‘ 800 å­—ç¬¦çª—å£å†…æœç´¢ streamType ä¸ backupUrlsï¼ˆç»éªŒè¶³å¤Ÿè¦†ç›– 1.txt çš„ç»“æ„ï¼‰
            window_l = max(0, start - 800)
            window_r = min(len(safe), end + 800)
            window = safe[window_l:window_r]

            # streamTypeï¼ˆå–å°±è¿‘çš„ä¸€ä¸ªæ•°å€¼ï¼‰
            st_match = re.search(r'"streamType"\s*:\s*(\d+)', window, re.IGNORECASE)
            try:
                stream_type = int(st_match.group(1)) if st_match else -1
            except Exception:
                stream_type = -1

            # backupUrlsï¼ˆå¯é€‰ï¼‰
            bu_list: List[str] = []
            bu_block = re.search(r'"backupUrls"\s*:\s*\[(.*?)\]', window, re.IGNORECASE | re.DOTALL)
            if bu_block:
                # æå–æ•°ç»„å†…çš„æ‰€æœ‰ URL
                for bu in re.findall(r'"(http[s]?:[^"\s]+?\.mp4)"', bu_block.group(1), re.IGNORECASE):
                    bu_list.append(bu)

            streams.append({
                "streamType": stream_type,
                "masterUrl": url,
                "backupUrls": bu_list
            })

        # å»é‡ï¼šæŒ‰ masterUrl å”¯ä¸€
        uniq: Dict[str, Dict[str, Any]] = {}
        for s in streams:
            u = s.get("masterUrl")
            if isinstance(u, str) and u and u not in uniq:
                uniq[u] = s
        return list(uniq.values())

    def _select_video_urls_by_rules(self, streams: List[Dict[str, Any]]) -> List[str]:
        """åº”ç”¨ä¸‹è½½é€‰æ‹©è§„åˆ™ï¼š
        - å¤šä¸ªä¸”åŒ…å« 259 -> ä¸ä¸‹è½½ï¼ˆè¿”å›ç©ºåˆ—è¡¨ï¼‰
        - å¤šä¸ªä¸”ä¸åŒ…å« 259 -> å…¨éƒ¨ä¸‹è½½ï¼ˆè¿”å›æ‰€æœ‰ masterUrlï¼‰
        - åªæœ‰ä¸€ä¸ª -> ç›´æ¥ä¸‹è½½ï¼ˆè¿”å›è¯¥ masterUrlï¼‰
        """
        if not isinstance(streams, list) or not streams:
            return []

        # è§„èŒƒåŒ–å¹¶æ”¶é›†
        urls: List[str] = []
        types: List[int] = []
        for s in streams:
            u = s.get("masterUrl")
            t = s.get("streamType")
            if isinstance(u, str) and u:
                urls.append(u)
            if isinstance(t, int):
                types.append(t)

        urls = list(dict.fromkeys(urls))  # å»é‡ä¿åº
        tset = set([t for t in types if isinstance(t, int)])

        if len(urls) >= 2:
            if 259 in tset:
                # å­˜åœ¨ 259 -> ä¸ä¸‹è½½
                return []
            # æ—  259 -> å…¨éƒ¨ä¸‹è½½
            return urls
        elif len(urls) == 1:
            return urls
        return []

    def get_selected_video_urls_from_page(self) -> List[str]:
        """å¯¹å½“å‰é¡µé¢è¿›è¡Œæºç çº§è§£æå¹¶æ ¹æ®è§„åˆ™è¿”å›åº”ä¸‹è½½çš„è§†é¢‘URLé›†åˆã€‚
        æ­¥éª¤ï¼špage_source -> æå– streams -> è§„åˆ™ç­›é€‰ -> è¿”å›URLåˆ—è¡¨ã€‚
        """
        try:
            html = self.driver.page_source or ""
        except Exception:
            html = ""
        streams = self._extract_master_streams_from_text(html)
        return self._select_video_urls_by_rules(streams)

    def _capture_api_headers_from_browser(self, keyword: str) -> Dict[str, str]:
        """åœ¨æµè§ˆå™¨ä¸­æ‰§è¡Œæœç´¢ï¼Œæ•è·APIè¯·æ±‚å¤´å’Œsearch_id"""
        try:
            # 1) é€šè¿‡UIè§¦å‘æœç´¢ï¼Œè´´è¿‘å®˜ç½‘æµç¨‹
            ok = self._perform_search_via_ui(keyword)
            if not ok:
                print("â„¹ï¸ å·²å›é€€ä¸ºç›´æ¥å¯¼èˆªåˆ°æœç´¢ç»“æœé¡µ")
                pass

            
            # è½®è¯¢è·å–æ€§èƒ½æ—¥å¿—ï¼ˆåœ¨æœç´¢è§¦å‘åæŒç»­ä¸€æ®µæ—¶é—´ï¼‰
            captured_filter = False  # æ˜¯å¦å·²æ•è· /search/filter çš„ç­¾åå¤´
            req_map: Dict[str, Dict[str, Any]] = {}

            def _is_target(u: str) -> bool:
                return isinstance(u, str) and (
                    '/api/sns/web/v1/search/notes' in u or '/api/sns/web/v1/search/filter' in u
                )

            # æ¢å¤å¤–å±‚è½®è¯¢ç»“æ„ï¼Œå®‰å…¨å¤„ç† performance æ—¥å¿—
            for _ in range(30):
                try:
                    logs = self.driver.get_log('performance') or []
                    for entry in logs:
                        try:
                            msg = json.loads(entry.get('message', '{}'))
                            message = msg.get('message', {})
                            method_type = message.get('method')
                            if not method_type:
                                continue

                            # ä»…å¤„ç†æ„Ÿå…´è¶£çš„äº‹ä»¶
                            if method_type not in [
                                'Network.requestWillBeSent',
                                'Network.requestWillBeSentExtraInfo',
                                'Network.responseReceived',
                                'Network.responseReceivedExtraInfo'
                            ]:
                                continue

                            params = message.get('params', {})
                            request_id = params.get('requestId') or params.get('requestId'.lower())

                            if method_type == 'Network.requestWillBeSent':
                                # è°ƒè¯•è®¡æ•°å·²ç§»é™¤
                                try:
                                    req = params.get('request', {})
                                    url = req.get('url', '')
                                    req_method = req.get('method', '')
                                    post_data = req.get('postData')
                                    if request_id:
                                        item = req_map.setdefault(request_id, {})
                                        item['url'] = url
                                        item['method'] = req_method
                                        if isinstance(post_data, str):
                                            item['postData'] = post_data
                                        item['ts'] = time.time()
                                        # æ–¹æ¡ˆBï¼šå‘½ä¸­ /search/filter æ—¶æå– keyword/search_id
                                        if isinstance(url, str) and '/search/filter' in url:
                                            try:
                                                parsed = urllib.parse.urlparse(url)
                                                qd = urllib.parse.parse_qs(parsed.query)
                                                kw = qd.get('keyword', [None])[0]
                                                sid = qd.get('search_id', [None])[0]
                                                # è‹¥ä¸ºPOSTåˆ™å°è¯•ä» body æå–
                                                if (not kw or not sid) and isinstance(post_data, str):
                                                    try:
                                                        body = json.loads(post_data)
                                                        if isinstance(body, dict):
                                                            kw = kw or body.get('keyword')
                                                            sid = sid or body.get('search_id')
                                                    except Exception:
                                                        pass
                                                if isinstance(kw, str) and isinstance(sid, str) and kw and sid:
                                                    self.filter_keyword = kw
                                                    self.filter_search_id = sid
                                                    self.filter_captured_at = time.time()
                                            except Exception:
                                                pass
                                except Exception:
                                    pass

                            elif method_type == 'Network.requestWillBeSentExtraInfo':
                                # è°ƒè¯•è®¡æ•°å·²ç§»é™¤
                                try:
                                    headers = params.get('headers', {})
                                    if headers and isinstance(headers, dict):
                                        # åˆå¹¶åˆ°req_map
                                        if request_id:
                                            item = req_map.setdefault(request_id, {})
                                            ih = item.get('headers', {})
                                            # ä»…ä¿ç•™éä¼ªé¦–éƒ¨
                                            ih.update({k: v for k, v in headers.items() if isinstance(k, str) and isinstance(v, str) and not k.strip().startswith(':')})
                                            item['headers'] = ih
                                            u = item.get('url', '')
                                            if isinstance(u, str) and _is_target(u):
                                                # ä»…åœ¨å‘½ä¸­ç›®æ ‡URLæ—¶æ›´æ–°æœ€è¿‘ç­¾åå¤´
                                                self.last_signed_headers = dict(ih)
                                                if '/search/filter' in u:
                                                    # æ–¹æ¡ˆBï¼šä»…åœ¨å‘½ä¸­ /search/filter æ—¶ç¼“å­˜å…¶ç­¾åå¤´
                                                    self.filter_signed_headers = dict(ih)
                                                    captured_filter = True
                                except Exception:
                                    pass

                            elif method_type == 'Network.responseReceived':
                                # è°ƒè¯•è®¡æ•°å·²ç§»é™¤
                                try:
                                    url = params.get('response', {}).get('url', '')
                                    if not isinstance(url, str) or not url:
                                        continue

                                    # å®˜æ–¹ search_id ä¼˜å…ˆä»URL
                                    try:
                                        parsed = urllib.parse.urlparse(url)
                                        q = urllib.parse.parse_qs(parsed.query)
                                        sid_url = q.get('search_id', [None])[0]
                                        if isinstance(sid_url, str) and sid_url and not self.official_search_id:
                                            self.official_search_id = sid_url
                                            if not self._printed_search_id:
                                                self._printed_search_id = True
                                    except Exception:
                                        pass

                                    # å…œåº•ï¼šè‹¥URLæœªå–åˆ°ä¸”å½“å‰ä¸ºnotesä¸”æœ‰postData
                                    try:
                                        if ('/search/notes' in url) and not self.official_search_id:
                                            candidate = req_map.get(request_id) or {}
                                            if candidate and candidate.get('postData'):
                                                body_candidate = candidate.get('postData')
                                                if isinstance(body_candidate, str) and body_candidate.strip():
                                                    bd_json = json.loads(body_candidate)
                                                    sid_body2 = bd_json.get('search_id') if isinstance(bd_json, dict) else None
                                                    if isinstance(sid_body2, str) and sid_body2:
                                                        self.official_search_id = sid_body2
                                                        if not self._printed_search_id:
                                                            self._printed_search_id = True
                                    except Exception:
                                        pass
                                except Exception:
                                    pass

                            elif method_type == 'Network.responseReceivedExtraInfo':
                                # å¯é€‰ï¼šè¡¥å…¨headers
                                try:
                                    headers = params.get('headers', {})
                                    if headers and request_id:
                                        item = req_map.setdefault(request_id, {})
                                        ih = item.get('headers', {})
                                        ih.update({k: v for k, v in headers.items() if isinstance(k, str) and isinstance(v, str)})
                                        item['headers'] = ih
                                except Exception:
                                    pass
                        except Exception:
                            continue

                    # è‹¥å·²æ•è· /search/filter çš„ç­¾åï¼Œä¸”å·²æœ‰æœ€è¿‘ç­¾åå¤´ï¼Œç»“æŸè½®è¯¢
                    if captured_filter and self.last_signed_headers:
                        break
                except Exception:
                    continue

                time.sleep(0.3)

            # åˆ é™¤è°ƒè¯•æ¨¡å¼ä¸‹çš„ç»Ÿè®¡è¾“å‡º
            if self.official_search_id and not self._printed_search_id:
                self._printed_search_id = True

            if self.cached_filter_groups:
                pass
            
            # æ–¹æ¡ˆS2ï¼šä¸å†è¿”å›æˆ–åˆå¹¶headers
            return {}
            
        except Exception as e:
            print(f"âŒ æ•è·APIè¯·æ±‚å¤´å¤±è´¥: {e}")
            return {}
    
    def _request_filters_via_api(self, keyword: str, search_id: str) -> List[Dict[str, Any]]:
        """æ–¹æ¡ˆAï¼šå±€éƒ¨ç›´è¿è°ƒè¯•ï¼Œä¸åšè§£æï¼Œä»…æ‰“å°åŸå§‹å“åº”ã€‚
        è¡Œä¸ºï¼š
        - ä¸èµ° make_api_requestï¼Œç›´æ¥ requests.get()
        - æ‰“å°çŠ¶æ€ç ã€å…³é”®ç­¾åå¤´é¢„è§ˆã€å“åº”ä½“å‰ 2000 å­—ç¬¦
        - ä¸è¿”å› filtersï¼Œä»…ç”¨äºè°ƒè¯•é˜¶æ®µ
        æ³¨æ„ï¼šä»…ç”¨äºè°ƒè¯•ï¼Œç”Ÿäº§é€»è¾‘ä¸ä¾èµ–è¯¥æ–¹æ³•ã€‚
        """
        try:
            # ä»ä¿ç•™åŒæºæ ¡éªŒï¼Œé¿å…è¯¯è¯·æ±‚
            if not isinstance(self.filter_signed_headers, dict) or not self.filter_signed_headers:
                print("â„¹ï¸ æœªæ•è· /search/filter ç­¾åå¤´ï¼Œè·³è¿‡ç›´è¿ï¼ˆè°ƒè¯•ï¼‰")
                return []
            if not (isinstance(self.filter_keyword, str) and self.filter_keyword and isinstance(self.filter_search_id, str) and self.filter_search_id):
                print("â„¹ï¸ æœªæ•è· /search/filter çš„ keyword/search_idï¼Œè·³è¿‡ç›´è¿ï¼ˆè°ƒè¯•ï¼‰")
                return []
            if keyword and self.filter_keyword and keyword != self.filter_keyword:
                print(f"â„¹ï¸ å½“å‰å…³é”®è¯ä¸ /search/filter åŒæºå…³é”®è¯ä¸ä¸€è‡´ï¼šcurrent='{keyword}' vs captured='{self.filter_keyword}'ï¼Œè·³è¿‡ç›´è¿ï¼ˆè°ƒè¯•ï¼‰")
                return []

            # ç»„è£…å®‰å…¨è¯·æ±‚å¤´ï¼šåŸºç¡€å¤´ + åŒæºç­¾åå¤´ï¼ˆfilter ä¸“ç”¨ä¼˜å…ˆï¼‰
            safe_headers = {k: v for k, v in self.headers.items() if isinstance(k, str) and not k.strip().startswith(':')}
            for k in ['x-s', 'x-t', 'x-s-common', 'x-b3-traceid', 'x-xray-traceid', 'referer', 'origin', 'user-agent']:
                v = self.filter_signed_headers.get(k) or self.last_signed_headers.get(k) if isinstance(self.last_signed_headers, dict) else None
                if isinstance(v, str) and v:
                    safe_headers[k] = v

            # æ„å»º GET URLï¼ˆä¸¥æ ¼åŒæºå‚æ•°ï¼‰
            filters_url = "https://edith.xiaohongshu.com/api/sns/web/v1/search/filter"
            qs = urlencode([("keyword", self.filter_keyword), ("search_id", self.filter_search_id)])
            url_qs = f"{filters_url}?{qs}"

            resp = requests.get(url_qs, headers=safe_headers, timeout=20)
            print(f"âœ… çŠ¶æ€ç : {resp.status_code}")

            try:
                full_text = (resp.text or '')
            except Exception:
                full_text = ''

            # æœ€å°è§£æï¼šåªè§£æ data.filtersï¼Œå¹¶æ˜ å°„ä¸º {id,name,filter_tags[{id,name}]} ç»“æ„
            try:
                raw = json.loads(full_text)
            except Exception:
                raw = None

            groups: List[Dict[str, Any]] = []
            try:
                if isinstance(raw, dict):
                    node = raw.get('data') if isinstance(raw.get('data'), dict) else raw
                    filters = node.get('filters') if isinstance(node, dict) else None
                    if isinstance(filters, list):
                        for g in filters:
                            if not isinstance(g, dict):
                                continue
                            gid = g.get('id') or ''
                            gname = g.get('name') or ''
                            tags = g.get('filter_tags') if isinstance(g.get('filter_tags'), list) else []
                            norm_tags = []
                            for t in tags:
                                if isinstance(t, dict):
                                    tid = t.get('id') or ''
                                    tname = t.get('name') or ''
                                    if tid and tname:
                                        norm_tags.append({'id': tid, 'name': tname})
                            if gid and gname and norm_tags:
                                groups.append({'id': gid, 'name': gname, 'filter_tags': norm_tags})

                print(f"âœ… è§£æåˆ° {len(groups)} ä¸ªåˆ†ç»„ï¼ˆæœ€å°è§£æï¼‰")
            except Exception as parse_err:
                print(f"âš ï¸ è§£æ data.filters å¤±è´¥ï¼š{parse_err}")
                groups = []

            return groups
        except Exception as e:
            print(f"âŒ è°ƒè¯•ç›´è¿å¤±è´¥: {e}")
            return []

    def _choose(self, prompt: str, options: List[str], default_idx: int = 0) -> str:
        """ç®€å•çš„äº¤äº’å¼é€‰æ‹©å™¨ï¼Œè¿”å›æ‰€é€‰é¡¹å­—ç¬¦ä¸²ã€‚"""
        if not options:
            return ""
        while True:
            print(prompt)
            for i, opt in enumerate(options):
                tag = "(é»˜è®¤)" if i == default_idx else ""
                print(f"  {i}. {opt} {tag}")
            s = input("è¯·è¾“å…¥åºå·ï¼ˆå›è½¦é»˜è®¤ï¼‰ï¼š").strip()
            if not s:
                return options[default_idx]
            if s.isdigit():
                idx = int(s)
                if 0 <= idx < len(options):
                    return options[idx]
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·é‡è¯•ã€‚")
    
    def _choose_tag_from_group(self, group: Dict[str, Any], default_idx: int = 0) -> str:
        """ä»åˆ†ç»„é‡Œé€‰æ‹©ä¸€ä¸ª tagï¼Œè¿”å›å…¶ idã€‚"""
        tags = group.get("filter_tags", [])
        if not tags:
            return ""
        gid = (group.get("id") or "").strip()

        # è®¡ç®—é»˜è®¤ç´¢å¼•ï¼šæŒ‰è§„åˆ™ä¸ºå„åˆ†ç»„å®šä½é»˜è®¤é¡¹
        def find_index_by(pred_name: str = None, pred_id: str = None) -> int:
            for i, t in enumerate(tags):
                tid = (t.get('id') or '').strip()
                tname = (t.get('name') or '').strip()
                if pred_id and tid == pred_id:
                    return i
                if pred_name and tname == pred_name:
                    return i
            return -1

        # è§„åˆ™ï¼š
        # - sort_type: é»˜è®¤ ç»¼åˆ(general)
        # - filter_note_type / filter_note_time / filter_note_range: é»˜è®¤ ä¸é™
        # - filter_hot: é»˜è®¤ä¸é€‰æ‹©ï¼ˆæä¾›è™šæ‹Ÿé¡¹â€œï¼ˆä¸é€‰æ‹©ï¼‰â€ï¼‰
        # è‹¥æ‰¾ä¸åˆ°åŒ¹é…é¡¹åˆ™ä¿ç•™ä¼ å…¥çš„ default_idx
        computed_default = default_idx
        if gid == 'sort_type':
            idx = find_index_by(pred_id='general')
            if idx < 0:
                idx = find_index_by(pred_name='ç»¼åˆ')
            if idx >= 0:
                computed_default = idx
        elif gid in ('filter_note_type', 'filter_note_time', 'filter_note_range'):
            idx = find_index_by(pred_name='ä¸é™')
            if idx < 0:
                idx = find_index_by(pred_id='ä¸é™')
            if idx >= 0:
                computed_default = idx

        # ä»…æ˜¾ç¤ºåç§°ï¼›ä¸ºçƒ­é—¨è¯åŠ å…¥â€œï¼ˆä¸é€‰æ‹©ï¼‰â€å¹¶é»˜è®¤ä¸é€‰
        names = [(t.get('name') or '').strip() for t in tags]
        skip_hot = (gid == 'filter_hot')
        if skip_hot:
            options = ['ï¼ˆä¸é€‰æ‹©ï¼‰'] + names
            # è¦†ç›–é»˜è®¤ç´¢å¼•åˆ° 0
            computed_default = 0
        else:
            options = names

        choice = self._choose(f"é€‰æ‹©{group.get('name','ç­›é€‰é¡¹')}ï¼š", options, default_idx=computed_default)

        # è§£æè¿”å›id
        if skip_hot and choice == 'ï¼ˆä¸é€‰æ‹©ï¼‰':
            return ""
        # æ˜ å°„æ˜¾ç¤ºåç§°åˆ°å¯¹åº”id
        chosen_name = choice
        for t in tags:
            if (t.get('name') or '').strip() == chosen_name:
                return (t.get('id') or '').strip()
        # æœªæ‰¾åˆ°åŒ¹é…åˆ™è¿”å›ç©ºï¼Œé¿å…è¯¯åŠ 
        return ""
    
    def _build_filters_interactive(self, keyword: str, search_id: str) -> Tuple[List[Dict[str, Any]], str, int]:
        """åŸºäºåŠ¨æ€æ¥å£äº¤äº’æ„é€  filtersï¼ŒåŒ…å« sort_typeã€note_typeã€timeã€rangeã€hotï¼Œæ’é™¤ä½ç½®è·ç¦»ã€‚"""
        # æ–¹æ¡ˆBï¼šä¼˜å…ˆç›´è¿ /search/filterï¼ˆä»…å½“å·²æ•è·è¯¥æ¥å£ç­¾åå¤´ï¼‰ï¼Œå¤±è´¥å›é€€CDPç¼“å­˜ï¼Œå†å›é€€é»˜è®¤åˆ†ç»„
        groups: List[Dict[str, Any]] = []
        if isinstance(self.filter_signed_headers, dict) and self.filter_signed_headers:
            groups = self._request_filters_via_api(keyword, search_id)
            if isinstance(groups, list) and groups:
                self.cached_filter_groups = groups
        if not groups and isinstance(self.cached_filter_groups, list) and self.cached_filter_groups:
            groups = self.cached_filter_groups
            print(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„ filter_groupsï¼ˆ{len(groups)} ç»„ï¼‰")
        wanted = {"sort_type", "filter_note_type", "filter_note_time", "filter_note_range", "filter_hot"}
        exclude = {"filter_pos_distance"}

        selected: List[Dict[str, Any]] = []
        chosen_sort = "general"  # é»˜è®¤å€¼
        chosen_note_type = 0     # é»˜è®¤å€¼

        print(f"\nğŸ¯ å¼€å§‹äº¤äº’å¼åˆ†ç±»é€‰æ‹©...")
        
        for g in groups:
            gid = g.get("id")
            group_name = g.get("name", "æœªçŸ¥åˆ†ç±»")
            
            # è·³è¿‡æ’é™¤çš„åˆ†ç±»
            if gid in exclude:
                print(f"â­ï¸ è·³è¿‡åˆ†ç±»: {group_name} ({gid})")
                continue
            
            # åªå¤„ç†éœ€è¦çš„åˆ†ç±»ï¼Œä½†æ˜¾ç¤ºæ‰€æœ‰åˆ†ç±»ä¾›å‚è€ƒ
            if gid not in wanted:
                print(f"â„¹ï¸ å¯é€‰åˆ†ç±»: {group_name} ({gid}) - æš‚ä¸æ”¯æŒäº¤äº’é€‰æ‹©")
                continue
            
            print(f"\nğŸ“‹ å½“å‰åˆ†ç±»: {group_name}")
            tag_id = self._choose_tag_from_group(g, default_idx=0)
            if not tag_id:
                continue
                
            selected.append({"tags": [tag_id], "type": gid})
            print(f"âœ… å·²é€‰æ‹©: {tag_id}")
            
            # å¤„ç†ç‰¹æ®Šé€»è¾‘
            if gid == "sort_type":
                chosen_sort = tag_id
            elif gid == "filter_note_type":
                # å¯¹é½é¡¶å±‚ note_typeï¼ˆ0 ä¸é™ï¼Œ1 è§†é¢‘ï¼Œ2 å›¾æ–‡ï¼‰
                if tag_id == "è§†é¢‘ç¬”è®°":
                    chosen_note_type = 1
                elif tag_id == "æ™®é€šç¬”è®°":
                    chosen_note_type = 2
                else:
                    chosen_note_type = 0

        print(f"\nğŸ‰ åˆ†ç±»é€‰æ‹©å®Œæˆï¼å…±é€‰æ‹©äº† {len(selected)} ä¸ªç­›é€‰æ¡ä»¶")
        return selected, chosen_sort, chosen_note_type

    def _request_page(self, keyword: str, page: int, search_id: str, sort: str = "general", note_type: int = 0, filters: List[Dict[str, Any]] = None, page_size: int = 20) -> List[Tuple[str, str]]:
        """æŒ‰é¡µè¯·æ±‚å¹¶è¿”å› (token, cleaned_id) åˆ—è¡¨"""
        payload = {
            "keyword": keyword,
            "page": page,
            "page_size": page_size,
            "search_id": search_id,
            "sort": sort,
            "note_type": note_type,
            "filters": filters or [],
            "ext_flags": [],
            "geo": "",
            "image_formats": ["jpg", "webp", "avif"],
        }
        
        # å°è¯•æœ€å¤š3æ¬¡è¯·æ±‚
        for attempt in range(3):
            data = self._make_api_request(self.search_url, method='POST', data=payload)
            if data:
                pairs = self._extract_token_id_pairs(data)
                cleaned: List[Tuple[str, str]] = []
                for token, nid in pairs:
                    cid = self._sanitize_note_id(nid)
                    if self._is_valid_note_id(cid):
                        cleaned.append((token, cid))
                return cleaned
            else:
                print(f"âš ï¸ ç¬¬ {attempt+1} æ¬¡è¯·æ±‚å¤±è´¥ï¼Œç­‰å¾…åé‡è¯•...")
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                
        print(f"âŒ ç¬¬ {page} é¡µè¯·æ±‚æœ€ç»ˆå¤±è´¥")
        return []
    
    def _crawl_notes(self, keyword: str, total_pages: int = 1, search_id: str = None, filters: List[Dict[str, Any]] = None, sort: str = "general", note_type: int = 0):
        """çˆ¬å–ç¬”è®°é“¾æ¥çš„ä¸»è¦é€»è¾‘"""
        print(f"\nğŸ” å¼€å§‹çˆ¬å–å…³é”®è¯: {keyword}")
        print(f"ğŸ“„ è®¡åˆ’çˆ¬å–é¡µæ•°: {total_pages}")
        print(f"ğŸ“Š æ’åºæ–¹å¼: {sort}")
        
        # ä»…é¦–æ¬¡æ‰“å° Search IDï¼Œé¿å…ä¸æ•è·é˜¶æ®µé‡å¤
        if isinstance(search_id, str) and search_id and not self._printed_search_id:
            print(f"ğŸ”‘ Search ID: {search_id}")
            self._printed_search_id = True
        
        # æ±‡æ€»ä¸å»é‡
        seen: set[Tuple[str, str]] = set()
        unique_pairs: List[Tuple[str, str]] = []

        for p in range(1, total_pages + 1):
            try:
                print(f"ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {p} é¡µ...")
                page_pairs = self._request_page(keyword, p, search_id, sort, note_type, filters)
                
                for token, nid in page_pairs:
                    key = (token, nid)
                    if key not in seen:
                        seen.add(key)
                        unique_pairs.append(key)
                        
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as page_err:
                print(f"âŒ ç¬¬ {p} é¡µè¯·æ±‚å¤±è´¥ï¼š{page_err}")

        # è¾“å‡ºç»“æœ
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print("="*50)
        base = "https://www.xiaohongshu.com/explore/{id}?xsec_token={token}&xsec_source=pc_search&source=web_search"
        
        for token, nid in unique_pairs:
            print(base.format(id=nid, token=token))
            
        print("="*50)
        print(f"ğŸ“Š æ€»è®¡å»é‡åï¼š{len(unique_pairs)} æ¡")
        print(f"ğŸ” å…³é”®è¯ï¼š{keyword}")
        print(f"ğŸ“Š æ’åºæ–¹å¼ï¼š{sort}")
        print(f"ğŸ“„ è¯·æ±‚é¡µæ•°ï¼š{total_pages}")
        
        return unique_pairs
    
    def _format_results(self, results: List[Tuple[str, str]]) -> List[str]:
        """æ ¼å¼åŒ–çˆ¬å–ç»“æœä¸ºURLåˆ—è¡¨"""
        base = "https://www.xiaohongshu.com/explore/{id}?xsec_token={token}&xsec_source=pc_search&source=web_search"
        return [base.format(id=nid, token=token) for token, nid in results]
    
    def run_interactive(self):
        """äº¤äº’å¼è¿è¡Œä¸»ç¨‹åº"""
        print("=== å°çº¢ä¹¦Seleniumé›†æˆçˆ¬è™« ===")
        print("ä¿æŒç™»å½•çŠ¶æ€ + APIè¯·æ±‚å¤´è·å– + é“¾æ¥çˆ¬å–\n")
        
        # 1. å¯åŠ¨æµè§ˆå™¨ - è®©æ‰€æœ‰Chromeæ—¥å¿—åœ¨è¿™é‡Œè¾“å‡ºå®Œæ¯•
        if not self._setup_browser():
            return False
        
        try:
            # 2. æ£€æŸ¥å¹¶ç¡®ä¿ç™»å½•
            if not self._ensure_login():
                return False
            
            # 3. ç­‰å¾…ä¸€ä¸‹è®©æ‰€æœ‰å¼‚æ­¥æ—¥å¿—è¾“å‡ºå®Œæ¯•
            time.sleep(1)
            
            # 4. è¾“å…¥å…³é”®è¯ - ç°åœ¨æ‰€æœ‰æ—¥å¿—éƒ½åº”è¯¥å·²ç»è¾“å‡ºå®Œäº†
            keyword = input("è¯·è¾“å…¥è¦çˆ¬å–çš„å…³é”®è¯: ").strip()
                
            if not keyword:
                print("âŒ å…³é”®è¯ä¸èƒ½ä¸ºç©º")
                return False
            
            # 5. å…ˆè¿›å…¥æœç´¢é¡µï¼Œå¹¶åœ¨è¯¥é¡µä¸Šä¸‹æ–‡æ•è·å®˜æ–¹ search_id
            if not self.official_search_id:
                # å¼ºåˆ¶è¦æ±‚å®˜æ–¹ search_idï¼Œä¸å†ä½¿ç”¨æœ¬åœ°ç”Ÿæˆ
                try:
                    _ = self._capture_api_headers_from_browser(keyword)
                except Exception:
                    pass
            if not self.official_search_id:
                print("âŒ æœªèƒ½æ•è·å®˜æ–¹ search_idï¼Œè¯·åœ¨æœç´¢ç»“æœé¡µå°è¯•è½»æ»šåŠ¨æˆ–åˆ‡æ¢ä¸€æ¬¡æ’åºåé‡è¯•ã€‚")
                return False
            search_id = self.official_search_id
            print(f"ğŸ†” ä½¿ç”¨å®˜æ–¹ search_id: {search_id}")
            
            # 6. åœ¨æœç´¢ç»“æœé¡µæå–è¯·æ±‚å¤´å’Œ Cookieï¼ˆç¬¦åˆä½ çš„æ—¶æœºè¦æ±‚ï¼‰
            if not self._extract_headers_from_browser():
                return False
            
            # 7. è·å–å¹¶é€‰æ‹©åŠ¨æ€åˆ†ç±»
            print(f"\nğŸ“Š æ­£åœ¨è·å–å…³é”®è¯ '{keyword}' çš„åŠ¨æ€åˆ†ç±»é€‰é¡¹...")
            filters, sort_type, note_type = self._build_filters_interactive(keyword, search_id)
            
            # 8. è¾“å…¥é¡µæ•°
            pages_input = input("\nè¯·è¾“å…¥è¦çˆ¬å–çš„é¡µæ•°ï¼ˆæ¯é¡µ20æ¡ï¼Œé»˜è®¤1é¡µï¼‰: ").strip()
            total_pages = int(pages_input) if pages_input.isdigit() else 1
            
            # 9. å¼€å§‹çˆ¬å–
            results = self._crawl_notes(keyword, total_pages, search_id, filters, sort_type, note_type)
            
            # 10. è¿”å›ç»“æœ
            if results:
                urls = self._format_results(results)
                print("\nâœ… çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
                print(f"ğŸ“‹ è·å–åˆ° {len(urls)} ä¸ªé“¾æ¥")
                # 10.1 è¯¢é—®æ˜¯å¦å¹¶å‘æŠ“å–è¯¦æƒ…é¡µ
                try:
                    do_detail = input("æ˜¯å¦å¹¶å‘æŠ“å–è¿™äº›è¯¦æƒ…é¡µå†…å®¹å¹¶ä¸‹è½½å›¾ç‰‡/è§†é¢‘ï¼Ÿ(Y/nï¼Œé»˜è®¤Y): ").strip().lower()
                except Exception:
                    do_detail = 'y'
                if do_detail in ('', 'y', 'yes', '1'):  # é»˜è®¤æ‰§è¡Œ
                    # é‡‡é›†å¹¶å‘ä¸é™é€Ÿå‚æ•°
                    try:
                        mw_s = input("å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤10ï¼‰: ").strip()
                        max_workers = int(mw_s) if mw_s.isdigit() and int(mw_s) > 0 else 10
                    except Exception:
                        max_workers = 10
                    try:
                        rp_s = input("æ¯ç§’æœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤50ï¼‰: ").strip()
                        rate_per_sec = int(rp_s) if rp_s.isdigit() and int(rp_s) > 0 else 50
                    except Exception:
                        rate_per_sec = 50
                    # é»˜è®¤åŒæ—¶ä¸‹è½½å›¾ç‰‡ä¸è§†é¢‘ï¼›è‹¥æ— å¯¹åº”èµ„æºåˆ™è‡ªåŠ¨è·³è¿‡
                    dl_images = True
                    dl_videos = True

                    # è¯¢é—®æ˜¯å¦ä»…ä¸‹è½½å‰ N æ¡ç”¨äºæµ‹è¯•
                    try:
                        n_input = input("ä»…ä¸‹è½½å‰ N æ¡è¯¦æƒ…é¡µç”¨äºæµ‹è¯•ï¼ˆå›è½¦=å…¨éƒ¨ï¼‰ï¼š").strip()
                        n_limit = int(n_input) if n_input.isdigit() and int(n_input) > 0 else None
                    except Exception:
                        n_limit = None
                    urls_to_fetch = urls[:n_limit] if n_limit else urls

                    downloader = XHSDetailDownloader(headers=self.headers,
                                                     max_workers=max_workers,
                                                     rate_per_sec=rate_per_sec,
                                                     dl_images=dl_images,
                                                     dl_videos=dl_videos)
                    summary = downloader.scrape_many(urls_to_fetch)
                    print(f"\nğŸ“¦ è¯¦æƒ…æŠ“å–å®Œæˆï¼šæˆåŠŸ {summary.get('ok',0)} æ¡ï¼Œå¤±è´¥ {summary.get('fail',0)} æ¡")

                return {
                    'urls': urls,
                    'results': results,
                    'keyword': keyword,
                    'sort': sort_type,
                    'note_type': note_type,
                    'filters': filters
                }
            else:
                print("\nâš ï¸ æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
                return None
            
            return True
            
        except Exception as e:
            print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False
            
        finally:
            # è¿è¡Œç»“æŸåè‡ªåŠ¨å…³é—­æµè§ˆå™¨ï¼ˆæŒ‰æ–¹æ¡ˆä¸€ï¼šä¸å†è¯¢é—®ï¼‰
            try:
                if self.driver:
                    self.driver.quit()
                    print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
            except Exception:
                pass
    

# =============== è¯¦æƒ…é¡µå¹¶å‘ä¸‹è½½ï¼ˆæ–¹æ¡ˆBï¼šåŒæ–‡ä»¶è½»é‡ç±»ï¼‰ ===============

class XHSDetailDownloader:
    """
    è½»é‡è¯¦æƒ…ä¸‹è½½å™¨ï¼šå¹¶å‘æŠ“å–å¤šä¸ªè¯¦æƒ…é¡µï¼Œè§£ææ ‡é¢˜/ä½œè€…/æ–‡æœ¬/å›¾ç‰‡/è§†é¢‘å¹¶ä¿å­˜ã€‚
    - å¤ç”¨ç™»å½•æ€ headersï¼ˆå« Cookie/UAï¼‰
    - çº¿ç¨‹å®‰å…¨é™é€Ÿï¼Œé»˜è®¤æ¯ç§’æœ€å¤š 50 æ¬¡è¯·æ±‚
    - å›¾ç‰‡å‘½åï¼šimg_001.jpgï¼›è§†é¢‘å‘½åï¼švideo_001.mp4
    - ç›®å½•å‘½åï¼šç¬”è®°ï¼ˆ<æ ‡é¢˜>ï¼‰
    """

    class _RateLimiter:
        def __init__(self, rate_per_sec: int):
            self.rate = max(1, int(rate_per_sec) if isinstance(rate_per_sec, int) else 50)
            self.lock = threading.Lock()
            self.q = deque()

        def wait(self):
            while True:
                now = time.time()
                with self.lock:
                    while self.q and now - self.q[0] >= 1.0:
                        self.q.popleft()
                    if len(self.q) < self.rate:
                        self.q.append(now)
                        return
                    sleep_for = 1.0 - (now - self.q[0])
                if sleep_for > 0:
                    time.sleep(min(sleep_for, 0.05))

    def __init__(self, headers: Dict[str, str], max_workers: int = 10, rate_per_sec: int = 50, dl_images: bool = True, dl_videos: bool = True, video_strategy: str | None = None):
        self.headers = headers or {}
        self.max_workers = max(1, int(max_workers) if isinstance(max_workers, int) else 10)
        self.rate_limiter = self._RateLimiter(rate_per_sec=rate_per_sec)
        self.dl_images = bool(dl_images)
        self.dl_videos = bool(dl_videos)
        self.video_strategy = video_strategy

        self.session = requests.Session()
        try:
            from requests.adapters import HTTPAdapter
            adapter = HTTPAdapter(pool_connections=self.max_workers, pool_maxsize=self.max_workers)
            self.session.mount('http://', adapter)
            self.session.mount('https://', adapter)
        except Exception:
            pass

    def _get(self, url: str, *, headers: dict = None, timeout: int = 20, stream: bool = False):
        self.rate_limiter.wait()
        return self.session.get(url, headers=headers or self.headers, timeout=timeout, stream=stream)

    @staticmethod
    def _ensure_dir(path: str):
        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)

    @staticmethod
    def _sanitize_dirname(name: str) -> str:
        if not name:
            return "æœªå‘½å"
        name = re.sub(r'[<>:"/\\|?*\n\r\t]', '_', name)
        name = name.strip().strip('.')
        return name[:100] if len(name) > 100 else name

    @staticmethod
    def _guess_ext_from_url(url: str) -> str:
        path = urllib.parse.urlsplit(url).path.lower()
        for ext in ('.jpg', '.jpeg', '.png', '.webp', '.gif'):
            if path.endswith(ext):
                return ext
        return '.jpg'

    def _fetch_html(self, url: str) -> str:
        resp = self._get(url, timeout=20)
        resp.raise_for_status()
        try:
            resp.encoding = resp.apparent_encoding or resp.encoding
        except Exception:
            pass
        return resp.text

    @staticmethod
    def _parse_title(html: str) -> str:
        m = re.search(r'<meta[^>]+(?:property|name)=["\']og:title["\'][^>]+content=["\'](.*?)["\']', html, flags=re.I | re.S)
        return m.group(1).strip() if m else ""

    @staticmethod
    def _parse_author(html: str) -> str:
        m = re.search(r'<span[^>]*class=["\']username["\'][^>]*>(.*?)</span>', html, flags=re.I | re.S)
        return m.group(1).strip() if m else ""

    @staticmethod
    def _parse_text_and_images(html: str):
        m = re.search(r'<meta[^>]+name=["\']description["\'][^>]+content=["\'](.*?)["\']', html, flags=re.I | re.S)
        text = m.group(1).strip() if m else ""
        imgs = re.findall(r'<meta[^>]+(?:property|name)=["\']og:image["\'][^>]+content=["\'](.*?)["\']', html, flags=re.I | re.S)
        seen = set()
        img_list = []
        for u in imgs:
            if u not in seen:
                seen.add(u)
                img_list.append(u)
        return text, img_list

    @staticmethod
    def _parse_video_urls(html: str):
        """
        ä»è¯¦æƒ…é¡µHTMLä¸­æå–æ‰€æœ‰è§†é¢‘masterUrlå€™é€‰ã€‚
        ä¸å‚è€ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼šä»…æ­£åˆ™æŠ“å–ï¼Œä¸åšç½‘ç»œè¯·æ±‚ã€‚
        """
        urls = re.findall(r'"masterUrl"\s*:\s*"(.*?)"', html, flags=re.I | re.S)
        return [XHSDetailDownloader._unescape_master_url(u) for u in urls if u]

    @staticmethod
    def _unescape_master_url(s: str) -> str:
        if not s:
            return s
        s = s.replace("\\u002F", "/").replace("\\u003d", "=").replace("\\u0026", "&")
        s = s.replace("\\/", "/")
        return s

    @staticmethod
    def _select_best_video_url(candidates: List[str]) -> str:
        """
        ä» candidates ä¸­é€‰æ‹©å•ä¸€æœ€ä½³å€™é€‰ï¼š
        1) å…ˆè¿‡æ»¤æ˜æ˜¾æ°´å°ï¼ˆæ›´ä¸¥æ ¼ï¼‰ï¼šwatermark/wm å‚æ•°ã€å…¸å‹æ°´å°è·¯å¾„å…³é”®è¯
        2) åœ¨éæ°´å°é›†åˆä¸­æŒ‰â€œå®é™…æ¸…æ™°åº¦çº¿ç´¢â€æ‹©ä¼˜ï¼š
           - ä¼˜å…ˆä» URL ä¸­è§£æåˆ†è¾¨ç‡ï¼ˆå¦‚ 2160/1440/1080/720 ç­‰æ•°å€¼ï¼‰è¿›è¡Œæ’åº
           - è‹¥æ— åˆ†è¾¨ç‡æ•°å€¼ï¼Œå†æŒ‰å…³é”®è¯ä¼˜å…ˆçº§ï¼šnowm/no_watermark/origin/src/uhd/4k/2k/1080/hd/720/540/480/360
        3) ä¸åšç½‘ç»œè¯·æ±‚ï¼ˆä¸æ¢æµ‹ Content-Lengthï¼‰ï¼Œä¿æŒè½»é‡
        4) è‹¥å…¨éƒ¨ä¸ºæ°´å°æˆ–æœªèƒ½è§£æï¼Œåˆ™å›é€€ä¸ºåŸå€™é€‰ä¸­çš„æœ€ä¼˜å…³é”®è¯
        """
        if not candidates:
            return ''
        # å»é‡ä¿åº
        seen = set()
        uniq = []
        for u in candidates:
            if u and u not in seen:
                seen.add(u)
                uniq.append(u)
        if not uniq:
            return ''

        def is_watermark(u: str) -> bool:
            s = u.lower()
            # å‚æ•°/å…³é”®è¯åˆ¤æ–­
            if ('watermark' in s) or ('wm=1' in s) or re.search(r'[?&#]watermark=1(?!\d)', s):
                return True
            # å¸¸è§è·¯å¾„/å…³é”®è¯ï¼ˆä¿å®ˆåŠ å…¥ï¼Œé¿å…è¯¯åˆ¤è¿‡å¤šï¼‰
            wm_keywords = ['mark=', 'logo=', '/watermark', 'withwm', 'wmvideo']
            return any(k in s for k in wm_keywords)

        def parse_resolution(u: str) -> int:
            """ä» URL çŒœæµ‹æ¸…æ™°åº¦ï¼Œè¿”å›åƒç´ é«˜åº¦ï¼ŒæœªçŸ¥è¿”å› -1"""
            s = u.lower()
            # å¸¸è§å½¢å¼ï¼š1080pã€720pã€_1080ã€/1080/ ç­‰
            m = re.search(r'(?:(?<=_)|(?<=/)|(?<=-)|(?<=\.)|^)(2160|1440|1080|960|720|540|480|360)(?:p)?(?=\D|$)', s)
            if m:
                try:
                    return int(m.group(1))
                except Exception:
                    pass
            return -1

        priority = [
            'nowm', 'no_watermark', 'origin', 'src', 'uhd', '4k', '2160', '2k',
            '1440', '1080', 'hd', '960', '720', '540', '480', '360'
        ]
        def kw_score(u: str) -> int:
            s = u.lower()
            for i, kw in enumerate(priority):
                if kw in s:
                    return len(priority) - i
            return 0

        # 1) è¿‡æ»¤æ°´å°
        non_wm = [u for u in uniq if not is_watermark(u)]

        def pick_best(pool: List[str]) -> str:
            if not pool:
                return ''
            # å…ˆæŒ‰åˆ†è¾¨ç‡æ•°å€¼æ’åºï¼ˆå¤§ä¼˜å…ˆï¼‰ï¼Œå†æŒ‰å…³é”®è¯å¾—åˆ†ï¼Œå†æŒ‰å‡ºç°é¡ºåº
            return max(pool, key=lambda x: (parse_resolution(x), kw_score(x), -pool.index(x)))
        
        # å…ˆåœ¨éæ°´å°é›†åˆä¸­é€‰æ‹©
        best_non_wm = pick_best(non_wm)
        if best_non_wm:
            return best_non_wm
        # å›é€€ï¼šåœ¨åŸé›†åˆä¸­æŒ‰å…³é”®è¯/åˆ†è¾¨ç‡é€‰ä¸€ä¸ª
        return pick_best(uniq)

    def _download_video(self, video_url: str, folder: str, *, filename: str | None = None, max_retries: int = 2):
        path = urllib.parse.urlsplit(video_url).path
        default_name = os.path.basename(path) or "video.mp4"
        if not default_name.lower().endswith('.mp4'):
            default_name = default_name + '.mp4'
        name = filename or default_name
        filepath = os.path.join(folder, name)
        for attempt in range(max_retries + 1):
            try:
                resp = self._get(video_url, timeout=30, stream=True)
                resp.raise_for_status()
                with open(filepath, 'wb') as f:
                    for chunk in resp.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                return filepath
            except Exception:
                if attempt >= max_retries:
                    return None
                time.sleep(0.5 * (attempt + 1))

    def _download_images_concurrent(self, image_urls: List[str], folder: str):
        if not image_urls:
            return []
        self._ensure_dir(folder)

        def _task(u, idx):
            try:
                resp = self._get(u, timeout=20)
                resp.raise_for_status()
                ext = self._guess_ext_from_url(u)
                filename = f"img_{idx:03d}{ext}"
                with open(os.path.join(folder, filename), 'wb') as f:
                    f.write(resp.content)
                return filename
            except Exception as e:
                return f"ERR:{idx}:{e}"

        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:
            futs = [ex.submit(_task, u, i + 1) for i, u in enumerate(image_urls)]
            for fu in as_completed(futs):
                try:
                    results.append(fu.result())
                except Exception:
                    results.append("ERR")
        return results

    def _download_videos_concurrent(self, video_urls: List[str], folder: str):
        if not video_urls:
            return []
        # å»é‡ä¿åº
        seen = set()
        uniq = []
        for u in video_urls:
            if u and u not in seen:
                seen.add(u)
                uniq.append(u)
        if not uniq:
            return []
        self._ensure_dir(folder)

        def _task(u, idx):
            fname = f"video_{idx:03d}.mp4"
            path = self._download_video(u, folder, filename=fname, max_retries=2)
            return path if path else f"ERR:{idx}"

        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:
            futs = [ex.submit(_task, u, i + 1) for i, u in enumerate(uniq)]
            for fu in as_completed(futs):
                try:
                    results.append(fu.result())
                except Exception:
                    results.append("ERR")
        return results

    def _save_content(self, out_dir: str, author: str, title: str, text: str) -> str:
        base_dir = os.path.dirname(__file__)
        note_dir = os.path.join(base_dir, out_dir)
        self._ensure_dir(note_dir)
        content_path = os.path.join(note_dir, "å†…å®¹.txt")
        with open(content_path, 'w', encoding='utf-8') as f:
            f.write(f"ä½œè€…ï¼š{author}\n")
            f.write(f"æ ‡é¢˜ï¼š{title}\n")
            f.write("æ–‡æœ¬ï¼š\n")
            f.write(text or "")
        return note_dir

    def _scrape_one(self, url: str) -> bool:
        try:
            html = self._fetch_html(url)
            title = self._parse_title(html)
            author = self._parse_author(html)
            text, images = self._parse_text_and_images(html)
            videos = self._parse_video_urls(html)

            safe_title = self._sanitize_dirname(title)
            out_dir = f"ç¬”è®°ï¼ˆ{safe_title}ï¼‰" if safe_title else "ç¬”è®°ï¼ˆæœªå‘½åï¼‰"
            note_dir = self._save_content(out_dir, author, title, text)
            img_ok = 0
            vid_ok = 0
            if self.dl_images and images:
                img_results = self._download_images_concurrent(images, note_dir)
                img_ok = sum(1 for r in img_results if r and not str(r).startswith('ERR:'))
            if self.dl_videos and videos:
                # ç»Ÿä¸€å»é‡ä¿åº
                seen_v = set()
                uniq_v = []
                for u in videos:
                    if u and u not in seen_v:
                        seen_v.add(u)
                        uniq_v.append(u)

                # æ–°è§„åˆ™ï¼š
                # - å¤šä¸ªä¸”åŒ…å« 259 -> ä»…ä¸‹è½½é _259 çš„æ‰€æœ‰è§†é¢‘
                # - å¤šä¸ªä¸”ä¸åŒ…å« 259 -> å…¨éƒ¨ä¸‹è½½
                # - ä»…ä¸€ä¸ª -> ç›´æ¥ä¸‹è½½
                def _is_259(u: str) -> bool:
                    s = (u or '').lower()
                    # ä»…åœ¨å­˜åœ¨ä¸‹åˆ’çº¿å‰ç¼€æ—¶è®¤å®šä¸º 259 æ¸…æ™°åº¦ï¼Œé¿å…è¯¯åˆ¤ query/id ä¸­çš„â€œ259â€
                    # ä¾‹å¦‚ï¼š..._259.mp4 æˆ– ..._259?...
                    return re.search(r'(?<=_)259(?=\D|$)', s) is not None

                if len(uniq_v) >= 2:
                    if any(_is_259(u) for u in uniq_v):
                        # å« _259ï¼šä»…ä¸‹è½½é _259 çš„æ‰€æœ‰è§†é¢‘
                        non_259 = [u for u in uniq_v if not _is_259(u)]
                        if non_259:
                            v_results = self._download_videos_concurrent(non_259, note_dir)
                            vid_ok = sum(1 for r in v_results if r and not str(r).startswith('ERR:'))
                        else:
                            vid_ok = 0
                    else:
                        v_results = self._download_videos_concurrent(uniq_v, note_dir)
                        vid_ok = sum(1 for r in v_results if r and not str(r).startswith('ERR:'))
                elif len(uniq_v) == 1:
                    path = self._download_video(uniq_v[0], note_dir, filename="video_001.mp4", max_retries=2)
                    vid_ok = 1 if path and os.path.exists(path) else 0

            abs_dir = os.path.abspath(note_dir)
            # è¾“å‡ºç²¾ç®€ä¸ºâ€œå¯¹åº”ç¬”è®°ä¸‹è½½æˆåŠŸâ€ï¼Œä¸å†æ‰“å°â€œè§†é¢‘æˆåŠŸâ€ç›¸å…³å­—æ ·
            print(f"âœ… ç¬”è®°ä¸‹è½½æˆåŠŸï¼š{title or 'æœªå‘½å'}")
            print(f"ğŸ“‚ ä¿å­˜è·¯å¾„ï¼š{abs_dir}")
            return True
        except Exception as e:
            print(f"âŒ è¯¦æƒ…æŠ“å–å¤±è´¥ï¼š{url} -> {e}")
            return False

    def scrape_many(self, urls: List[str]) -> Dict[str, int]:
        if not urls:
            return {'ok': 0, 'fail': 0}
        ok = 0
        fail = 0
        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:
            futs = {ex.submit(self._scrape_one, u): u for u in urls}
            for fu in as_completed(futs):
                try:
                    if fu.result():
                        ok += 1
                    else:
                        fail += 1
                except Exception:
                    fail += 1
        return {'ok': ok, 'fail': fail}


def main():
    """ä¸»å‡½æ•°"""
    # åœ¨ç¨‹åºå¼€å§‹æ—¶å°±é‡å®šå‘stderrï¼Œå½»åº•å±è”½æ‰€æœ‰Chromeæ—¥å¿—
    original_stderr = sys.stderr
    try:
        # åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„stderrè¿‡æ»¤å™¨ï¼Œåªå…è®¸æˆ‘ä»¬çš„ç¨‹åºè¾“å‡º
        class FilteredStderr:
            def __init__(self, original):
                self.original = original
                
            def write(self, text):
                # è¿‡æ»¤æ‰Chromeç›¸å…³çš„æ—¥å¿—
                if any(keyword in text for keyword in [
                    'WARNING: All log messages before absl::InitializeLog()',
                    'voice_transcription.cc',
                    'DevTools listening on',
                    'Registering VoiceTranscriptionCapability'
                ]):
                    return  # ä¸è¾“å‡ºè¿™äº›æ—¥å¿—
                return self.original.write(text)
                
            def flush(self):
                return self.original.flush()
        
        # åº”ç”¨è¿‡æ»¤å™¨
        sys.stderr = FilteredStderr(original_stderr)
        
        crawler = XHSIntegratedCrawler()
        crawler.run_interactive()
        
    finally:
        # æ¢å¤åŸå§‹stderr
        sys.stderr = original_stderr

if __name__ == "__main__":
    main()
 
